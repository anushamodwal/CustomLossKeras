{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Implement a classifier that takes as input raw images from the camera and is able to classify the\n",
    "following three classes:<br>\n",
    "• Bicycles:    0 <br>\n",
    "• Pedestrians: 1<br>\n",
    "• Noise:       2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Report</h1>\n",
    "\n",
    "The data set consists of 30,000 images equally labelled for three classes in which it needs to be classified.<br>\n",
    "\n",
    "A 4 layer fully connected neural network classifier is built using Keras to classify the images into classes.\n",
    "10% of data is taken as test data and it is kept in mind that all classes have equal amount of in data when this division is made thus using <b>stratify</b> parameter while performing the split.<br>\n",
    "\n",
    "The model uses the following metrics:\n",
    "\n",
    "<b>Loss Metric: categorical_crossentropy</b> as a loss metric to measure the performance of classification model whose output is a in the range of probability values of 0 and 1.<br>\n",
    "\n",
    "<b>Optimizer: Adam</b>\n",
    "\n",
    "<b>Metric: Accuracy</b> \n",
    "\n",
    "The resultant model has the accuracy of 90.43%.\n",
    "\n",
    "<b>Evaluation Metrics of the Model:</b>\n",
    "\n",
    "Recall, Precision and F1 Score where examined together with confusion matrix to proceed furthur it is clear that Recall of class 1 i.e pedestrian has to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Prioritizing One Class</H2>\n",
    "\n",
    "In order to fit into a situation of never missing out a pedestrian, a <b>custom loss function</b> was used for loss metric to penalize miscalsification of the pedestrian class.\n",
    "\n",
    "The custom fuction gives higher weight to the class in question and thus reduces misclassification error to almost zero.\n",
    "\n",
    "This however also takes a toll on accuracy and misclassification of other classes increases, however for demonstration purposes this code snippet justifies how this can be done.\n",
    "\n",
    "It is also clear that the class weights require tuning before a the final values can be concluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 30000\n",
      "Feature−vector dimension: 54\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = np.load('dataset.npy').item()\n",
    "\n",
    "# Some info on the dataset\n",
    "dataset_size = len(dataset['x'])\n",
    "feature_vector_dimen = len(dataset['x'][1, :])\n",
    "no_classes = len(set(dataset['y']))\n",
    "print(\"Dataset size: %d\" % dataset_size)\n",
    "print(\"Feature−vector dimension: %d\" % feature_vector_dimen)\n",
    "print(\"Number of classes: %d\" % no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique, counts = np.unique(dataset['y'], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes have equal amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform name species into one hot vector encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataset['y'])\n",
    "Y = encoder.transform(dataset['y'])\n",
    "Y = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['x'], Y, test_size=0.1, random_state=123, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(loss_metric):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = feature_vector_dimen , activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss = loss_metric, optimizer = 'adam' , metrics = ['accuracy'])\n",
    "\n",
    "    # Train model (use 10% of training set as validation set)\n",
    "    history = model.fit(X_train, y_train, validation_split=0.1, epochs = 15, batch_size=16)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "    y_p = model.predict(X_test)\n",
    "    print(confusion_matrix(y_test.argmax(axis=1), y_p.argmax(axis=1)))\n",
    "    \n",
    "    print(classification_report(y_test.argmax(axis=1), y_p.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24300 samples, validate on 2700 samples\n",
      "Epoch 1/15\n",
      "24300/24300 [==============================] - 2s 99us/step - loss: 1.2570 - acc: 0.7524 - val_loss: 0.5105 - val_acc: 0.7785\n",
      "Epoch 2/15\n",
      "24300/24300 [==============================] - 2s 84us/step - loss: 0.4237 - acc: 0.8229 - val_loss: 0.3793 - val_acc: 0.8378\n",
      "Epoch 3/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.3413 - acc: 0.8535 - val_loss: 0.3152 - val_acc: 0.8596\n",
      "Epoch 4/15\n",
      "24300/24300 [==============================] - 2s 95us/step - loss: 0.3027 - acc: 0.8672 - val_loss: 0.3345 - val_acc: 0.8422\n",
      "Epoch 5/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2865 - acc: 0.8759 - val_loss: 0.2763 - val_acc: 0.8793\n",
      "Epoch 6/15\n",
      "24300/24300 [==============================] - 2s 82us/step - loss: 0.2814 - acc: 0.8795 - val_loss: 0.2646 - val_acc: 0.8852\n",
      "Epoch 7/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2669 - acc: 0.8876 - val_loss: 0.2653 - val_acc: 0.8867\n",
      "Epoch 8/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2645 - acc: 0.8862 - val_loss: 0.2819 - val_acc: 0.8678\n",
      "Epoch 9/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2611 - acc: 0.8892 - val_loss: 0.2452 - val_acc: 0.8989\n",
      "Epoch 10/15\n",
      "24300/24300 [==============================] - 3s 104us/step - loss: 0.2588 - acc: 0.8896 - val_loss: 0.2492 - val_acc: 0.8915\n",
      "Epoch 11/15\n",
      "24300/24300 [==============================] - 2s 98us/step - loss: 0.2544 - acc: 0.8916 - val_loss: 0.2465 - val_acc: 0.8937\n",
      "Epoch 12/15\n",
      "24300/24300 [==============================] - 2s 85us/step - loss: 0.2540 - acc: 0.8914 - val_loss: 0.2410 - val_acc: 0.8907\n",
      "Epoch 13/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2513 - acc: 0.8937 - val_loss: 0.2531 - val_acc: 0.8878\n",
      "Epoch 14/15\n",
      "24300/24300 [==============================] - 2s 82us/step - loss: 0.2467 - acc: 0.8953 - val_loss: 0.2699 - val_acc: 0.8793\n",
      "Epoch 15/15\n",
      "24300/24300 [==============================] - 2s 83us/step - loss: 0.2502 - acc: 0.8926 - val_loss: 0.2497 - val_acc: 0.9007\n",
      "3000/3000 [==============================] - 0s 20us/step\n",
      "\n",
      "acc: 90.43%\n",
      "[[898 101   1]\n",
      " [155 816  29]\n",
      " [  0   1 999]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.90      0.87      1000\n",
      "          1       0.89      0.82      0.85      1000\n",
      "          2       0.97      1.00      0.98      1000\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_model('categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom Loss Function<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function with costs\n",
    "\n",
    "# def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "#     nb_cl = weights.shape[1]\n",
    "#     final_mask = K.zeros_like(y_pred[:, 0])\n",
    "#     y_pred_max = K.max(y_pred, axis=1)\n",
    "#     y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "#     y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n",
    "#     for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "#         final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "#     return K.categorical_crossentropy(y_true,y_pred) * final_mask\n",
    "\n",
    "def w_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * y_pred * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncce = w_categorical_crossentropy(weights=[4.0,14.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24300 samples, validate on 2700 samples\n",
      "Epoch 1/15\n",
      "24300/24300 [==============================] - 4s 185us/step - loss: -4.7691 - acc: 0.5334 - val_loss: -4.8894 - val_acc: 0.5130\n",
      "Epoch 2/15\n",
      "24300/24300 [==============================] - 3s 132us/step - loss: -4.7900 - acc: 0.5162 - val_loss: -4.9148 - val_acc: 0.5285\n",
      "Epoch 3/15\n",
      "24300/24300 [==============================] - 3s 109us/step - loss: -4.8117 - acc: 0.5520 - val_loss: -4.9151 - val_acc: 0.5385\n",
      "Epoch 4/15\n",
      "24300/24300 [==============================] - 3s 106us/step - loss: -4.8048 - acc: 0.5397 - val_loss: -4.9296 - val_acc: 0.5433\n",
      "Epoch 5/15\n",
      "24300/24300 [==============================] - 3s 109us/step - loss: -4.8080 - acc: 0.5980 - val_loss: -4.9192 - val_acc: 0.5715\n",
      "Epoch 6/15\n",
      "24300/24300 [==============================] - 3s 108us/step - loss: -4.8073 - acc: 0.5917 - val_loss: -4.8948 - val_acc: 0.6104\n",
      "Epoch 7/15\n",
      "24300/24300 [==============================] - 3s 121us/step - loss: -4.7597 - acc: 0.6118 - val_loss: -4.9275 - val_acc: 0.5552\n",
      "Epoch 8/15\n",
      "24300/24300 [==============================] - 3s 115us/step - loss: -4.8203 - acc: 0.5535 - val_loss: -4.9297 - val_acc: 0.5437\n",
      "Epoch 9/15\n",
      "24300/24300 [==============================] - 3s 135us/step - loss: -4.8149 - acc: 0.5820 - val_loss: -4.9221 - val_acc: 0.5800\n",
      "Epoch 10/15\n",
      "24300/24300 [==============================] - 3s 118us/step - loss: -4.7514 - acc: 0.5070 - val_loss: -4.8708 - val_acc: 0.4748\n",
      "Epoch 11/15\n",
      "24300/24300 [==============================] - 3s 115us/step - loss: -4.7927 - acc: 0.5201 - val_loss: -4.8462 - val_acc: 0.5900\n",
      "Epoch 12/15\n",
      "24300/24300 [==============================] - 3s 118us/step - loss: -4.7516 - acc: 0.6097 - val_loss: -4.8193 - val_acc: 0.6256\n",
      "Epoch 13/15\n",
      "24300/24300 [==============================] - 3s 121us/step - loss: -4.6940 - acc: 0.6321 - val_loss: -4.7966 - val_acc: 0.6319\n",
      "Epoch 14/15\n",
      "24300/24300 [==============================] - 3s 119us/step - loss: -4.8055 - acc: 0.5577 - val_loss: -4.9047 - val_acc: 0.5715\n",
      "Epoch 15/15\n",
      "24300/24300 [==============================] - 3s 116us/step - loss: -4.7621 - acc: 0.4740 - val_loss: -4.7804 - val_acc: 0.3796\n",
      "3000/3000 [==============================] - 0s 29us/step\n",
      "\n",
      "acc: 37.63%\n",
      "[[   0 1000    0]\n",
      " [   0  999    1]\n",
      " [   0  870  130]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1000\n",
      "          1       0.35      1.00      0.52      1000\n",
      "          2       0.99      0.13      0.23      1000\n",
      "\n",
      "avg / total       0.45      0.38      0.25      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amodwal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "build_model(ncce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
